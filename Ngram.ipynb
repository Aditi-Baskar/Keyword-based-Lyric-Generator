{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwM2Gc24kNOE",
        "outputId": "50773314-eadf-413a-b14f-6d34a3c068a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk.util import everygrams\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all the lyrics\n",
        "f = open('ts_data.json')\n",
        "data = json.load(f)\n",
        "t=0\n",
        "all_lyrics = []\n",
        "for i in data['songs']:\n",
        "  all_lyrics.append(i['lyrics'])"
      ],
      "metadata": {
        "id": "1gn3gK8wpLY6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BR50sO_-u_3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all the sentences of the entire lyrics\n",
        "sentences = []\n",
        "for i in all_lyrics:\n",
        "  sentences.extend(i.split('\\n'))"
      ],
      "metadata": {
        "id": "qWilL2_i367H"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "vocab = set()\n",
        "for sentence in sentences:\n",
        "  tokens = word_tokenize(sentence.lower())\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  text.append(tokens)\n",
        "  vocab.update(tokens)\n",
        "vocab1 = list(vocab)"
      ],
      "metadata": {
        "id": "l92flCEX6VXM"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ngrams = (everygrams(i,max_len=6) for i in text)"
      ],
      "metadata": {
        "id": "LZkVD-N3jfnN"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = MLE(order=6)\n",
        "#train, vocab = padded_everygram_pipeline(order=4,text=text)\n",
        "lm.fit(all_ngrams,vocab1)"
      ],
      "metadata": {
        "id": "FCGhMWsm70ii"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm.generate(num_words=30,text_seed=['let','me','love','you'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYv54JCj-PBs",
        "outputId": "2ab57fa7-0539-4ab2-ebbf-e52f70bd7abc"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['let',\n",
              " 'me',\n",
              " 'want',\n",
              " 'you',\n",
              " 'more',\n",
              " 'than',\n",
              " 'i',\n",
              " 'know',\n",
              " 'how',\n",
              " 'to',\n",
              " 'mention',\n",
              " 'him',\n",
              " 'was',\n",
              " 'like',\n",
              " 'wishing',\n",
              " 'you',\n",
              " 'never',\n",
              " 'found',\n",
              " 'out',\n",
              " 'that',\n",
              " 'love',\n",
              " 'could',\n",
              " 'be',\n",
              " 'that',\n",
              " 'strong',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'let',\n",
              " 'you',\n",
              " 'go']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenize(\"Hello ny name is yolo\\n whar ae, yiu doing's\")"
      ],
      "metadata": {
        "id": "bCo6VgLvmZZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174dc03a-616c-4d77-90b3-2bfe8b5511e0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'ny', 'name', 'is', 'yolo', 'whar', 'ae', ',', 'yiu', 'doing', \"'s\"]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp1 = \"Hello ny name is yolo\\n whar ae, yiu doing's\"\n",
        "temp1.split(' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JurMG-mEAz-K",
        "outputId": "9b45d44c-5eeb-4c5a-8620-17558f5441c7"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'ny', 'name', 'is', 'yolo\\n', 'whar', 'ae,', 'yiu', \"doing's\"]"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = open('trial.txt','w')\n",
        "print(all_lyrics[0][0:80])\n",
        "s = all_lyrics[0][0:80]\n",
        "f1.write(s)\n",
        "all_lyrics[0][0:80]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "vVJ0s36iCZiC",
        "outputId": "6b5ae767-7034-404e-f942-624da760ea56"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I walked through the door with you, the air was cold\n",
            "But somethin' 'bout it felt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I walked through the door with you, the air was cold\\nBut somethin' 'bout it felt\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab = set()\n",
        "all_text = []\n",
        "for i in all_lyrics:\n",
        "  all_tokens = []\n",
        "  for sentence in i.split('\\n'):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens.append('\\n')\n",
        "    new_vocab.update(tokens)\n",
        "    all_tokens.extend(tokens)\n",
        "  all_text.append(all_tokens)\n",
        "vocab2 = list(new_vocab)\n",
        "new_ngrams = (everygrams(i,max_len=6) for i in all_text)"
      ],
      "metadata": {
        "id": "aj1UMvPsCzbD"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_text[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfpxo8SVGjlZ",
        "outputId": "364327d5-d2e2-45c1-ab53-69e807f068e5"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'walked', 'through', 'the', 'door', 'with', 'you', 'the', 'air', 'was', 'cold', '\\n', 'But', 'somethin', 'it', 'felt', 'like', 'home', 'somehow', '\\n', 'And', 'I', 'left', 'my', 'scarf', 'there', 'at', 'your', 'sister', 'house', '\\n', 'And', 'you', 'still', 'got', 'it', 'in', 'your', 'drawer', 'even', 'now', '\\n', '\\n', 'Oh', 'your', 'sweet', 'disposition', 'and', 'my', 'gaze', '\\n', 'We', 'singin', 'in', 'the', 'car', 'getting', 'lost', 'upstate', '\\n', 'Autumn', 'leaves', 'fallin', 'down', 'like', 'pieces', 'into', 'place', '\\n', 'And', 'I', 'can', 'picture', 'it', 'after', 'all', 'these', 'days', '\\n', '\\n', 'And', 'I', 'know', 'it', 'long', 'gone', 'and', '\\n', 'That', 'magic', 'not', 'here', 'no', 'more', '\\n', 'And', 'I', 'might', 'be', 'okay', 'but', 'I', 'not', 'fine', 'at', 'all', '\\n', 'Oh', 'oh', 'oh', '\\n', '\\n', 'there', 'we', 'ar', 'again', 'on', 'that', 'little', 'town', 'street', '\\n', 'You', 'almost', 'ran', 'the', 'red', 'you', 'were', 'lookin', 'over', 'at', 'me', '\\n', 'Wind', 'in', 'my', 'hair', 'I', 'was', 'there', '\\n', 'I', 'remember', 'it', 'all', 'too', 'well', '\\n', '\\n', 'Photo', 'album', 'on', 'the', 'counter', 'your', 'cheeks', 'were', 'turnin', 'red', '\\n', 'You', 'used', 'to', 'be', 'a', 'little', 'kid', 'with', 'glasses', 'in', 'a', 'bed', '\\n', 'And', 'your', 'mother', 'tellin', 'stories', 'you', 'on', 'the', 'team', '\\n', 'You', 'taught', 'me', 'your', 'past', 'thinkin', 'your', 'future', 'was', 'me', '\\n', 'And', 'you', 'were', 'tossing', 'me', 'the', 'car', 'keys', 'Fuck', 'the', 'patriarchy', '\\n', 'Keychain', 'on', 'the', 'ground', 'we', 'were', 'always', 'skippin', 'town', '\\n', 'And', 'I', 'was', 'thinkin', 'on', 'the', 'drive', 'down', 'Any', 'time', 'now', '\\n', 'He', 'gon', 'na', 'say', 'it', 'love', 'you', 'never', 'called', 'it', 'what', 'it', 'was', '\\n', 'we', 'were', 'dead', 'and', 'gone', 'and', 'buried', '\\n', 'Check', 'the', 'pulse', 'and', 'come', 'back', 'swearin', 'it', 'the', 'same', '\\n', 'After', 'three', 'months', 'in', 'the', 'grave', '\\n', 'And', 'then', 'you', 'wondered', 'where', 'it', 'went', 'to', 'as', 'I', 'reached', 'for', 'you', '\\n', 'But', 'all', 'I', 'felt', 'was', 'shame', 'and', 'you', 'held', 'my', 'lifeless', 'frame', '\\n', 'See', 'Taylor', 'Swift', 'LiveGet', 'tickets', 'as', 'low', 'as', 'might', 'also', 'like', '\\n', 'And', 'I', 'know', 'it', 'long', 'gone', 'and', '\\n', 'There', 'was', 'nothing', 'else', 'I', 'could', 'do', '\\n', 'And', 'I', 'forget', 'about', 'you', 'long', 'enough', '\\n', 'To', 'forget', 'why', 'I', 'needed', 'to', '\\n', '\\n', 'there', 'we', 'are', 'again', 'in', 'the', 'middle', 'of', 'the', 'night', '\\n', 'We', 'dancin', 'the', 'kitchen', 'in', 'the', 'refrigerator', 'light', '\\n', 'Down', 'the', 'stairs', 'I', 'was', 'there', '\\n', 'I', 'remember', 'it', 'all', 'too', 'well', '\\n', 'And', 'there', 'we', 'are', 'again', 'when', 'nobody', 'had', 'to', 'know', '\\n', 'You', 'kept', 'me', 'like', 'a', 'secret', 'but', 'I', 'kept', 'you', 'like', 'an', 'oath', '\\n', 'Sacred', 'prayer', 'and', 'we', 'swear', '\\n', 'To', 'remember', 'it', 'all', 'too', 'well', 'yeah', '\\n', '\\n', 'Well', 'maybe', 'we', 'got', 'lost', 'in', 'translation', 'maybe', 'I', 'asked', 'for', 'too', 'much', '\\n', 'But', 'maybe', 'this', 'thing', 'was', 'a', 'masterpiece', 'you', 'tore', 'it', 'all', 'up', '\\n', 'Runnin', 'scared', 'I', 'was', 'there', '\\n', 'I', 'remember', 'it', 'all', 'too', 'well', '\\n', 'And', 'you', 'call', 'me', 'up', 'again', 'just', 'to', 'break', 'me', 'like', 'a', 'promise', '\\n', 'So', 'casually', 'cruel', 'in', 'the', 'name', 'of', 'bein', 'honest', '\\n', 'I', 'a', 'piece', 'of', 'paper', 'lyin', 'here', '\\n', 'I', 'remember', 'it', 'all', 'all', 'all', '\\n', 'They', 'say', 'all', 'well', 'that', 'ends', 'well', 'but', 'I', 'in', 'a', 'new', 'hell', '\\n', 'Every', 'time', 'you', 'my', 'mind', '\\n', 'You', 'said', 'if', 'we', 'had', 'been', 'closer', 'in', 'age', 'maybe', 'it', 'would', 'been', 'fine', '\\n', 'And', 'that', 'made', 'me', 'want', 'to', 'die', '\\n', 'The', 'idea', 'you', 'had', 'of', 'me', 'who', 'was', 'she', '\\n', 'A', 'jewel', 'whose', 'shine', 'reflects', 'on', 'you', '\\n', 'Not', 'weepin', 'in', 'a', 'party', 'bathroom', '\\n', 'Some', 'actress', 'askin', 'me', 'what', 'happened', 'you', '\\n', 'That', 'what', 'happened', 'you', '\\n', 'You', 'who', 'charmed', 'my', 'dad', 'with', 'jokes', '\\n', 'Sippin', 'coffee', 'like', 'you', 'on', 'a', 'show', '\\n', 'But', 'then', 'he', 'watched', 'me', 'watch', 'the', 'front', 'door', 'all', 'night', 'willin', 'you', 'to', 'come', '\\n', 'And', 'he', 'said', 'It', 'supposed', 'to', 'be', 'fun', 'turning', '\\n', '\\n', 'Time', 'wo', 'fly', 'it', 'like', 'I', 'paralyzed', 'by', 'it', '\\n', 'I', 'like', 'to', 'be', 'my', 'old', 'self', 'again', 'but', 'I', 'still', 'tryin', 'to', 'find', 'it', '\\n', 'After', 'plaid', 'shirt', 'days', 'and', 'nights', 'when', 'you', 'made', 'me', 'your', 'own', '\\n', 'Now', 'you', 'mail', 'back', 'my', 'things', 'and', 'I', 'walk', 'home', 'alone', '\\n', 'But', 'you', 'keep', 'my', 'old', 'scarf', 'from', 'that', 'very', 'first', 'week', '\\n', 'it', 'reminds', 'you', 'of', 'innocence', 'and', 'it', 'smells', 'like', 'me', '\\n', 'You', 'ca', 'get', 'rid', 'of', 'it', '\\n', 'you', 'remember', 'it', 'all', 'too', 'well', 'yeah', '\\n', '\\n', 'there', 'we', 'are', 'again', 'when', 'I', 'loved', 'you', 'so', '\\n', 'Back', 'before', 'you', 'lost', 'the', 'one', 'real', 'thing', 'you', 'ever', 'known', '\\n', 'It', 'was', 'rare', 'I', 'was', 'there', '\\n', 'I', 'remember', 'it', 'all', 'too', 'well', '\\n', 'Wind', 'in', 'my', 'hair', 'you', 'were', 'there', '\\n', 'You', 'remember', 'it', 'all', '\\n', 'Down', 'the', 'stairs', 'you', 'were', 'there', '\\n', 'You', 'remember', 'it', 'all', '\\n', 'It', 'was', 'rare', 'I', 'was', 'there', '\\n', 'I', 'remember', 'it', 'all', 'too', 'well', '\\n', 'And', 'I', 'was', 'never', 'good', 'at', 'tellin', 'jokes', 'but', 'the', 'punch', 'line', 'goes', '\\n', 'I', 'get', 'older', 'but', 'your', 'lovers', 'stay', 'my', 'age', '\\n', 'From', 'when', 'your', 'Brooklyn', 'broke', 'my', 'skin', 'and', 'bones', '\\n', 'I', 'a', 'soldier', 'who', 'returning', 'half', 'her', 'weight', '\\n', 'And', 'did', 'the', 'twin', 'flame', 'bruise', 'paint', 'you', 'blue', '\\n', 'Just', 'between', 'us', 'did', 'the', 'love', 'affair', 'maim', 'you', 'too', '\\n', 'in', 'this', 'city', 'barren', 'cold', '\\n', 'I', 'still', 'remember', 'the', 'first', 'fall', 'of', 'snow', '\\n', 'And', 'how', 'it', 'glistened', 'as', 'it', 'fell', '\\n', 'I', 'remember', 'it', 'all', 'too', 'well', '\\n', '\\n', 'Just', 'between', 'us', 'did', 'the', 'love', 'affair', 'maim', 'you', 'all', 'too', 'well', '\\n', 'Just', 'between', 'us', 'do', 'you', 'remember', 'it', 'all', 'too', 'well', '\\n', 'Just', 'between', 'us', 'I', 'remember', 'it', 'Just', 'between', 'us', 'all', 'too', 'well', '\\n', 'Wind', 'in', 'my', 'hair', 'I', 'was', 'there', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'Down', 'the', 'stairs', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'Sacred', 'prayer', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'It', 'was', 'rare', 'you', 'remember', 'it', 'all', 'too', 'well', '\\n', 'Wind', 'in', 'my', 'hair', 'I', 'was', 'there', 'I', 'was', 'there', 'Oh', '\\n', 'Down', 'the', 'stairs', 'I', 'was', 'there', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'Sacred', 'prayer', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'It', 'was', 'rare', 'you', 'remember', 'it', 'All', 'too', 'well', '\\n', 'Wind', 'in', 'my', 'hair', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'Down', 'the', 'stairs', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'Sacred', 'prayer', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'It', 'was', 'rare', 'you', 'remember', 'it', '\\n', 'Wind', 'in', 'my', 'hair', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'Down', 'the', 'stairs', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'Sacred', 'prayer', 'I', 'was', 'there', 'I', 'was', 'there', '\\n', 'It', 'was', 'rare', 'you', 'remember', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lm1 = MLE(order=6)\n",
        "#train, vocab = padded_everygram_pipeline(order=4,text=text)\n",
        "lm1.fit(new_ngrams,vocab2)\n",
        "pretext = ['let','me','love','you']\n",
        "generated_lyrics = lm1.generate(num_words=100,text_seed=['let','me','love','you'],random_seed=2)\n",
        "temp_string = ''\n",
        "for word in pretext:\n",
        "  temp_string += word + ' '\n",
        "for word in generated_lyrics:\n",
        "  if(word!='\\n'):\n",
        "    temp_string += word + ' '\n",
        "  else:\n",
        "    temp_string += word\n",
        "print(temp_string)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gLTaZMxGAgF",
        "outputId": "dfb37de1-cdba-4795-97aa-fb45c8780f25"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "let me love you let me want you \n",
            "You just see right through me \n",
            "But if you only knew me \n",
            "We could be a beautiful miracle unbelievable \n",
            "Instead of just invisible \n",
            "\n",
            "Oh yeah oh \n",
            "\n",
            "There a fire inside of you \n",
            "Anytime you feel danger or fear \n",
            "Then instantly \n",
            "I will appear \n",
            "\n",
            "I every woman \n",
            "It all in me yeah oh yeah \n",
            "I can read your thoughts right now \n",
            "Every one from A to Z \n",
            "\n",
            "I can cast a spell \n",
            "See but you ca tell \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = ['how','are','you','doing','this']\n",
        "random_generated = lm1.generate(num_words=100,text_seed=['how','are','you','doing','this'],random_seed=2)\n",
        "temp_string = ''\n",
        "for word in sample_text:\n",
        "  temp_string += word + ' '\n",
        "for word in random_generated:\n",
        "  if(word!='\\n'):\n",
        "    temp_string += word + ' '\n",
        "  else:\n",
        "    temp_string += word\n",
        "print(temp_string)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZNLksoCGR9K",
        "outputId": "6c9b005b-9891-4193-b078-a7afe89ec427"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "how are you doing this week \n",
            "Jealousy misery \n",
            "Gon give you what you gave to me \n",
            "\n",
            "Baby baby \n",
            "\n",
            "Oh and give you what you gave to me \n",
            "\n",
            "\n",
            "A string that pulled me \n",
            "Out of all the bricks they threw at me At me \n",
            "And every day is like a battle Every day is like a battle \n",
            "But every night with us is like a dream \n",
            "Baby we the new romantics \n",
            "The best people in life are \n",
            "Singer turned gangster \n",
            "You do want to fight me \n",
            "Straight to the \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bBy57MeKXpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
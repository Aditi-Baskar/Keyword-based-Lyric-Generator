{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwM2Gc24kNOE",
        "outputId": "50773314-eadf-413a-b14f-6d34a3c068a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk.util import everygrams\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all the lyrics\n",
        "f = open('ts_data.json')\n",
        "data = json.load(f)\n",
        "t=0\n",
        "all_lyrics = []\n",
        "for i in data['songs']:\n",
        "  all_lyrics.append(i['lyrics'])"
      ],
      "metadata": {
        "id": "1gn3gK8wpLY6"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BR50sO_-u_3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all the sentences of the entire lyrics\n",
        "sentences = []\n",
        "for i in all_lyrics:\n",
        "  sentences.extend(i.split('\\n'))"
      ],
      "metadata": {
        "id": "qWilL2_i367H"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "vocab = set()\n",
        "for sentence in sentences:\n",
        "  tokens = word_tokenize(sentence.lower())\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  text.append(tokens)\n",
        "  vocab.update(tokens)\n",
        "vocab1 = list(vocab)"
      ],
      "metadata": {
        "id": "l92flCEX6VXM"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ngrams = (everygrams(i,max_len=6) for i in text)"
      ],
      "metadata": {
        "id": "LZkVD-N3jfnN"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = MLE(order=6)\n",
        "#train, vocab = padded_everygram_pipeline(order=4,text=text)\n",
        "lm.fit(all_ngrams,vocab1)"
      ],
      "metadata": {
        "id": "FCGhMWsm70ii"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm.generate(num_words=30,text_seed=['let','me','love','you'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYv54JCj-PBs",
        "outputId": "2ab57fa7-0539-4ab2-ebbf-e52f70bd7abc"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['let',\n",
              " 'me',\n",
              " 'want',\n",
              " 'you',\n",
              " 'more',\n",
              " 'than',\n",
              " 'i',\n",
              " 'know',\n",
              " 'how',\n",
              " 'to',\n",
              " 'mention',\n",
              " 'him',\n",
              " 'was',\n",
              " 'like',\n",
              " 'wishing',\n",
              " 'you',\n",
              " 'never',\n",
              " 'found',\n",
              " 'out',\n",
              " 'that',\n",
              " 'love',\n",
              " 'could',\n",
              " 'be',\n",
              " 'that',\n",
              " 'strong',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'let',\n",
              " 'you',\n",
              " 'go']"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCo6VgLvmZZq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
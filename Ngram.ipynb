{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwM2Gc24kNOE",
        "outputId": "cb6cd6d3-83e8-475e-fb75-4dfd3885cd1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import MLE\n",
        "from nltk.util import everygrams\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all the lyrics\n",
        "f = open('ts_data.json')\n",
        "data = json.load(f)\n",
        "t=0\n",
        "all_lyrics = []\n",
        "for i in data['songs']:\n",
        "  all_lyrics.append(i['lyrics'])"
      ],
      "metadata": {
        "id": "1gn3gK8wpLY6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BR50sO_-u_3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a list of all the sentences of the entire lyrics\n",
        "sentences = []\n",
        "for i in all_lyrics:\n",
        "  sentences.extend(i.split('\\n'))"
      ],
      "metadata": {
        "id": "qWilL2_i367H"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = []\n",
        "vocab = set()\n",
        "for sentence in sentences:\n",
        "  tokens = word_tokenize(sentence.lower())\n",
        "  tokens = [word for word in tokens if word.isalpha()]\n",
        "  text.append(tokens)\n",
        "  vocab.update(tokens)\n",
        "vocab1 = list(vocab)"
      ],
      "metadata": {
        "id": "l92flCEX6VXM"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ngrams = (everygrams(i,max_len=6) for i in text)"
      ],
      "metadata": {
        "id": "LZkVD-N3jfnN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm = MLE(order=6)\n",
        "#train, vocab = padded_everygram_pipeline(order=4,text=text)\n",
        "lm.fit(all_ngrams,vocab1)"
      ],
      "metadata": {
        "id": "FCGhMWsm70ii"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lm.generate(num_words=30,text_seed=['let','me','love','you'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LYv54JCj-PBs",
        "outputId": "3b4d6ed0-030a-4206-ed2d-5c70d8901c86"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['kiss',\n",
              " 'you',\n",
              " 'in',\n",
              " 'a',\n",
              " 'crowded',\n",
              " 'room',\n",
              " 'in',\n",
              " 'a',\n",
              " 'crowded',\n",
              " 'room',\n",
              " 'in',\n",
              " 'a',\n",
              " 'crowded',\n",
              " 'room',\n",
              " 'in',\n",
              " 'a',\n",
              " 'crowded',\n",
              " 'room',\n",
              " 'in',\n",
              " 'a',\n",
              " 'crowded',\n",
              " 'room',\n",
              " 'in',\n",
              " 'a',\n",
              " 'crowded',\n",
              " 'room',\n",
              " 'in',\n",
              " 'a',\n",
              " 'crowded',\n",
              " 'room']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f1 = open('trial.txt','w')\n",
        "print(all_lyrics[0][0:80])\n",
        "s = all_lyrics[0][0:80]\n",
        "f1.write(s)\n",
        "all_lyrics[0][0:80]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "vVJ0s36iCZiC",
        "outputId": "6b5ae767-7034-404e-f942-624da760ea56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I walked through the door with you, the air was cold\n",
            "But somethin' 'bout it felt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I walked through the door with you, the air was cold\\nBut somethin' 'bout it felt\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_vocab = set()\n",
        "all_text = []\n",
        "for i in all_lyrics:\n",
        "  all_tokens = []\n",
        "  for sentence in i.split('\\n'):\n",
        "    tokens = word_tokenize(sentence)\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    tokens.append('\\n')\n",
        "    new_vocab.update(tokens)\n",
        "    all_tokens.extend(tokens)\n",
        "  all_text.append(all_tokens)\n",
        "vocab2 = list(new_vocab)\n",
        "new_ngrams = (everygrams(i,max_len=6) for i in all_text)"
      ],
      "metadata": {
        "id": "aj1UMvPsCzbD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# To train the model for a value n\n",
        "def train_model(n):\n",
        "  new_ngrams = (everygrams(i,max_len=n) for i in all_text)\n",
        "  lm1 = MLE(order=n)\n",
        "  lm1.fit(new_ngrams,vocab2)\n",
        "  return lm1\n"
      ],
      "metadata": {
        "id": "Yy7SkNQhF3Ko"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(6)\n",
        "# To generate lyric give a pretext and just call the function generate_lyric\n",
        "pretext = ['let','me','love','you']\n",
        "def generate_lyric(pretext):\n",
        "  generated_lyrics = model.generate(num_words=100,text_seed=pretext,random_seed=2)\n",
        "  temp_string = ''\n",
        "  for word in pretext:\n",
        "    temp_string += word + ' '\n",
        "  for word in generated_lyrics:\n",
        "    if(word!='\\n'):\n",
        "      temp_string += word + ' '\n",
        "    else:\n",
        "      temp_string += word\n",
        "  print(temp_string)\n",
        "  \n",
        "generate_lyric(pretext)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y4HdUcNUHu7L",
        "outputId": "8b7d1335-9ab3-438c-b5ed-27444304cbfd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "let me love you let me want you \n",
            "You just see right through me \n",
            "But if you only knew me \n",
            "We could be a beautiful miracle unbelievable \n",
            "Instead of just invisible \n",
            "\n",
            "Oh yeah oh \n",
            "\n",
            "There a fire inside of you \n",
            "Anytime you feel danger or fear \n",
            "Then instantly \n",
            "I will appear \n",
            "\n",
            "I every woman \n",
            "It all in me yeah oh yeah \n",
            "I can read your thoughts right now \n",
            "Every one from A to Z \n",
            "\n",
            "I can cast a spell \n",
            "See but you ca tell \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0bBy57MeKXpQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}